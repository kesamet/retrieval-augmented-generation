## experimental

- `corrective_rag.py`: Implementation of [corrective retrieval augmented generation](https://github.com/HuskyInSalt/CRAG) with langgraph.
- `eval_truchain.py`, `eval_trullama.py`: Using [trulens-eval](https://github.com/truera/trulens) to evaluate and track LLM experiments. The demos here are using gemini-pro as an alternative to OpenAI models.
- `trace_phoenix.py`: Using [arize-phoenix](https://github.com/Arize-ai/phoenix) to trace through the execution of LLM Application to understand the internals and to troubleshoot problems related to things like retrieval and tool execution.
